{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.assembly_graph\n",
    "import lib.plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_seed(seed, depth):\n",
    "    right, left = lib.assembly_graph.build_full_from_seed_graph(seed)\n",
    "    depth = pd.Series(lib.assembly_graph.add_reverse_complement_depth(depth)).astype(float)\n",
    "    assert (depth.index.isin(right.keys()) | depth.index.isin(left.keys())).all()\n",
    "    dgraph = pd.DataFrame(np.zeros((len(depth), len(depth))), index=depth.index, columns=depth.index)\n",
    "    for unitig in depth.index:\n",
    "        dgraph.loc[unitig, right[unitig]] = 1\n",
    "    return dgraph, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_messages(dgraph, depth):\n",
    "    # Step -1\n",
    "    send_to_r = dgraph\n",
    "    send_to_l = dgraph.T\n",
    "    total_from_l = send_to_r.sum()\n",
    "    total_from_r = send_to_l.sum()\n",
    "    send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "    send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "    send_to_r = send_to_r_next.fillna(0)\n",
    "    send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "    # Step 0\n",
    "    total_from_l = send_to_r.sum()\n",
    "    total_from_r = send_to_l.sum()\n",
    "    send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "    send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "    send_to_r = send_to_r_next.fillna(0)\n",
    "    send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "    return send_to_r, send_to_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_messages(send_to_r, send_to_l, depth, gamma=1.0):\n",
    "    total_from_l = send_to_r.sum()\n",
    "    total_from_r = send_to_l.sum()\n",
    "    # Update depth\n",
    "    next_depth = (total_from_r + total_from_l + (gamma * depth)) / (2 + gamma)\n",
    "    # Scale the depth so there's no overall loss.\n",
    "    depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "    # Calculate next message\n",
    "    send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "    send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "    send_to_r = send_to_r_next.fillna(0)\n",
    "    send_to_l = send_to_l_next.fillna(0)\n",
    "    return send_to_r, send_to_l, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tall saw-horse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    'AACCG': ['ACCGG'],\n",
    "    'ACCGG': ['CCGGG', 'CCGGA'],\n",
    "    'TACCG': ['ACCGG'],\n",
    "    'TAACC': ['AACCG'],\n",
    "    'TTACC': ['TACCG'],\n",
    "    'CCGGG': ['CGGGT'],\n",
    "    'CCGGA': ['CGGAT'],\n",
    "}\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    'AACCG': 9,\n",
    "    'ACCGG': 10,\n",
    "    'CCGGG': 9,\n",
    "    'CCGGA': 1,\n",
    "    'TACCG': 1,\n",
    "    'TAACC': 9,\n",
    "    'TTACC': 1,\n",
    "    'CGGGT': 9,\n",
    "    'CGGAT': 1,\n",
    "})\n",
    "dgraph, depth0 = build_from_seed(seed, observed_depth)\n",
    "send_to_r, send_to_l = initialize_messages(dgraph, depth0)\n",
    "\n",
    "depth = depth0\n",
    "thresh = 1e-5\n",
    "i = 0\n",
    "while True:\n",
    "    send_to_r, send_to_l, new_depth = iterate_messages(send_to_r, send_to_l, depth)\n",
    "    delta = new_depth - depth\n",
    "    change = np.sqrt(np.sum(np.square(new_depth - depth)))\n",
    "    depth = new_depth\n",
    "    if change < thresh:\n",
    "        print(f\"Converged on iteration {i}\")\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "sns.heatmap(send_to_r + send_to_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saw-horse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    'AACCG': ['ACCGG'],\n",
    "    'ACCGG': ['CCGGG', 'CCGGA'],\n",
    "    'TACCG': ['ACCGG'],\n",
    "}\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    'AACCG': 3,\n",
    "    'ACCGG': 4,\n",
    "    'CCGGG': 3,\n",
    "    'CCGGA': 1,\n",
    "    'TACCG': 1,\n",
    "})\n",
    "dgraph, depth0 = build_from_seed(seed, observed_depth)\n",
    "send_to_r, send_to_l = initialize_messages(dgraph, depth0)\n",
    "\n",
    "depth = depth0\n",
    "thresh = 1e-5\n",
    "i = 0\n",
    "while True:\n",
    "    send_to_r, send_to_l, new_depth = iterate_messages(send_to_r, send_to_l, depth)\n",
    "    delta = new_depth - depth\n",
    "    change = np.sqrt(np.sum(np.square(new_depth - depth)))\n",
    "    depth = new_depth\n",
    "    if change < thresh:\n",
    "        print(f\"Converged on iteration {i}\")\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "# sns.heatmap(send_to_r + send_to_l)\n",
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0 = pd.Series(observed_depth).astype(float)\n",
    "n = len(depth0)\n",
    "depth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Directed\" graph with each node pointing to nodes that are downstream.\n",
    "# If I want a graph of each node pointing upstream, that's just dgraph.T\n",
    "# After the *first* step, where I distribute each nodes depth equally upstream\n",
    "# and downstream, I'm going to need two contingency tables, because\n",
    "# uflow != dflow.T\n",
    "\n",
    "linked_5p_to_3p = downstream\n",
    "\n",
    "dgraph_5p_to_3p = pd.DataFrame(np.zeros((n, n)), index=depth0.index, columns=depth0.index)\n",
    "for k in dgraph_5p_to_3p:\n",
    "    dgraph_5p_to_3p.loc[k, linked_5p_to_3p[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_contingency = dgraph_5p_to_3p.copy()\n",
    "r_contingency = dgraph_5p_to_3p.T.copy()\n",
    "\n",
    "sns.heatmap(l_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "send_to_r = r_contingency\n",
    "send_to_l = l_contingency\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "depth = depth0\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "# Initialization done\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converge first message\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# Graveyard of depths past.\n",
    "# depth = np.sqrt(np.mean((np.array([total_from_r, total_from_l, depth]) + 1)**2))  #  (depth0 * gamma + depth + total_from_r + total_from_l) / (3 + gamma)\n",
    "# next_depth = np.cbrt((total_from_r + pc) * (total_from_l + pc) * (depth + pc)) - pc\n",
    "# depth = (\n",
    "#     np.exp(\n",
    "#         (\n",
    "#             np.log(total_from_r + pc) +\n",
    "#             np.log(total_from_l + pc) +\n",
    "#             np.log(depth + pc)\n",
    "#         ) / (3 + pc)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Update depth\n",
    "gamma = 1\n",
    "next_depth = (total_from_r + total_from_l + (gamma * depth)) / (2 + gamma)\n",
    "# Scale the depth so there's no overall loss.\n",
    "depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "# Step done\n",
    "\n",
    "# sns.heatmap(send_to_r)\n",
    "# plt.plot(depth)\n",
    "# plt.ylim(0)\n",
    "depth\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle w/ Switch-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    'ACCCG': ['CCCGG'],\n",
    "    'CCCGG': ['CCGGT'],\n",
    "    'CCGGT': ['CGGTA'],\n",
    "    'CGGTA': ['GGTAC'],\n",
    "    'GGTAC': ['GTACC'],\n",
    "    'GTACC': ['TACCC'],\n",
    "    'TACCC': ['ACCCG'],\n",
    "}\n",
    "downstream, upstream = lib.assembly_graph.build_full_from_seed_graph(seed)\n",
    "\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    'ACCCG': 1,\n",
    "    'CCCGG': 1,\n",
    "    'CCGGT': 1,\n",
    "    'CGGTA': 1,\n",
    "    'GGTAC': 1,\n",
    "    'GTACC': 1,\n",
    "    'TACCC': 1,\n",
    "})\n",
    "\n",
    "assert lib.assembly_graph.mapping_all_upstream(upstream)\n",
    "upstream, observed_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0 = pd.Series(observed_depth).astype(float)\n",
    "n = len(depth0)\n",
    "depth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Directed\" graph with each node pointing to nodes that are downstream.\n",
    "# If I want a graph of each node pointing upstream, that's just dgraph.T\n",
    "# After the *first* step, where I distribute each nodes depth equally upstream\n",
    "# and downstream, I'm going to need two contingency tables, because\n",
    "# uflow != dflow.T\n",
    "\n",
    "linked_5p_to_3p = downstream\n",
    "\n",
    "dgraph_5p_to_3p = pd.DataFrame(np.zeros((n, n)), index=depth0.index, columns=depth0.index)\n",
    "for k in dgraph_5p_to_3p:\n",
    "    dgraph_5p_to_3p.loc[k, linked_5p_to_3p[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_contingency = dgraph_5p_to_3p.copy()\n",
    "r_contingency = dgraph_5p_to_3p.T.copy()\n",
    "\n",
    "sns.heatmap(l_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "send_to_r = r_contingency\n",
    "send_to_l = l_contingency\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "depth = depth0\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "# Initialization done\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converge first message\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# Graveyard of depths past.\n",
    "# depth = np.sqrt(np.mean((np.array([total_from_r, total_from_l, depth]) + 1)**2))  #  (depth0 * gamma + depth + total_from_r + total_from_l) / (3 + gamma)\n",
    "# next_depth = np.cbrt((total_from_r + pc) * (total_from_l + pc) * (depth + pc)) - pc\n",
    "# depth = (\n",
    "#     np.exp(\n",
    "#         (\n",
    "#             np.log(total_from_r + pc) +\n",
    "#             np.log(total_from_l + pc) +\n",
    "#             np.log(depth + pc)\n",
    "#         ) / (3 + pc)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Update depth\n",
    "gamma = 1\n",
    "next_depth = (total_from_r + total_from_l + (gamma * depth)) / (2 + gamma)\n",
    "# Scale the depth so there's no overall loss.\n",
    "depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "# Step done\n",
    "\n",
    "# sns.heatmap(send_to_r)\n",
    "# plt.plot(depth)\n",
    "# plt.ylim(0)\n",
    "depth\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six-cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    'ACCCG': ['CCCGG'],\n",
    "    'CCCGG': ['CCGGA'],\n",
    "    'CCGGA': ['CGGAC'],\n",
    "    'CGGAC': ['GGACC'],\n",
    "    'GGACC': ['GACCC'],\n",
    "    'GACCC': ['ACCCG']\n",
    "}\n",
    "downstream, upstream = lib.assembly_graph.build_full_from_seed_graph(seed)\n",
    "\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    'ACCCG': 1,\n",
    "    'CCCGG': 1,\n",
    "    'CCGGA': 2,\n",
    "    'CGGAC': 1,\n",
    "    'GGACC': 1,\n",
    "    'GACCC': 1,\n",
    "})\n",
    "\n",
    "assert lib.assembly_graph.mapping_all_upstream(upstream)\n",
    "upstream, observed_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0 = pd.Series(observed_depth).astype(float)\n",
    "n = len(depth0)\n",
    "depth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Directed\" graph with each node pointing to nodes that are downstream.\n",
    "# If I want a graph of each node pointing upstream, that's just dgraph.T\n",
    "# After the *first* step, where I distribute each nodes depth equally upstream\n",
    "# and downstream, I'm going to need two contingency tables, because\n",
    "# uflow != dflow.T\n",
    "\n",
    "linked_5p_to_3p = downstream\n",
    "\n",
    "dgraph_5p_to_3p = pd.DataFrame(np.zeros((n, n)), index=depth0.index, columns=depth0.index)\n",
    "for k in dgraph_5p_to_3p:\n",
    "    dgraph_5p_to_3p.loc[k, linked_5p_to_3p[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_contingency = dgraph_5p_to_3p.copy()\n",
    "r_contingency = dgraph_5p_to_3p.T.copy()\n",
    "\n",
    "sns.heatmap(l_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "send_to_r = r_contingency\n",
    "send_to_l = l_contingency\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "depth = depth0\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "# Initialization done\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converge first message\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not finding the first message\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# Graveyard of depths past.\n",
    "# depth = np.sqrt(np.mean((np.array([total_from_r, total_from_l, depth]) + 1)**2))  #  (depth0 * gamma + depth + total_from_r + total_from_l) / (3 + gamma)\n",
    "# next_depth = np.cbrt((total_from_r + pc) * (total_from_l + pc) * (depth + pc)) - pc\n",
    "# depth = (\n",
    "#     np.exp(\n",
    "#         (\n",
    "#             np.log(total_from_r + pc) +\n",
    "#             np.log(total_from_l + pc) +\n",
    "#             np.log(depth + pc)\n",
    "#         ) / (3 + pc)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Update depth\n",
    "gamma = 1.0\n",
    "nu = 0.\n",
    "\n",
    "next_depth = (total_from_r + total_from_l + (gamma * depth) + (nu * depth0)) / (2 + gamma + nu)\n",
    "# Scale the depth so there's no overall loss.\n",
    "depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "# Step done\n",
    "\n",
    "# sns.heatmap(send_to_r)\n",
    "# plt.plot(depth)\n",
    "# plt.ylim(0)\n",
    "depth\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double-six-cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    # Top cycle\n",
    "    'GGACC': ['GACCC'],\n",
    "    'GACCC': ['ACCCG'],\n",
    "    'ACCCG': ['CCCGG'],\n",
    "    'CCCGG': ['CCGGA'],\n",
    "    'CCGGA': ['CGGAC'],\n",
    "    \n",
    "    # Link\n",
    "    'CGGAC': ['GGACC', 'GGACT'],\n",
    "    \n",
    "    # Bottom cycle\n",
    "    'GGACT': ['GACTC'],\n",
    "    'GACTC': ['ACTCG'],\n",
    "    'ACTCG': ['CTCGG'],\n",
    "    'CTCGG': ['TCGGA'],\n",
    "    'TCGGA': ['CGGAC'],\n",
    "    \n",
    "}\n",
    "downstream, upstream = lib.assembly_graph.build_full_from_seed_graph(seed)\n",
    "\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    # Top cycle\n",
    "    'GGACC': 1,\n",
    "    'GACCC': 1,\n",
    "    'ACCCG': 1,\n",
    "    'CCCGG': 1,\n",
    "    'CCGGA': 1,\n",
    "    \n",
    "    # Link\n",
    "    'CGGAC': 3,\n",
    "    \n",
    "    # Bottom \n",
    "    'GGACT': 2, \n",
    "    'GACTC': 2,\n",
    "    'ACTCG': 2,\n",
    "    'CTCGG': 2,\n",
    "    'TCGGA': 2,\n",
    "})\n",
    "\n",
    "assert lib.assembly_graph.mapping_all_upstream(upstream)\n",
    "upstream, observed_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0 = pd.Series(observed_depth).astype(float)\n",
    "n = len(depth0)\n",
    "depth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Directed\" graph with each node pointing to nodes that are downstream.\n",
    "# If I want a graph of each node pointing upstream, that's just dgraph.T\n",
    "# After the *first* step, where I distribute each nodes depth equally upstream\n",
    "# and downstream, I'm going to need two contingency tables, because\n",
    "# uflow != dflow.T\n",
    "\n",
    "linked_5p_to_3p = downstream\n",
    "\n",
    "dgraph_5p_to_3p = pd.DataFrame(np.zeros((n, n)), index=depth0.index, columns=depth0.index)\n",
    "for k in dgraph_5p_to_3p:\n",
    "    dgraph_5p_to_3p.loc[k, linked_5p_to_3p[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_contingency = dgraph_5p_to_3p.copy()\n",
    "r_contingency = dgraph_5p_to_3p.T.copy()\n",
    "\n",
    "sns.heatmap(l_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "send_to_r = r_contingency\n",
    "send_to_l = l_contingency\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "depth = depth0\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "# Initialization done\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converge first message\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not finding the first message\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# Graveyard of depths past.\n",
    "# depth = np.sqrt(np.mean((np.array([total_from_r, total_from_l, depth]) + 1)**2))  #  (depth0 * gamma + depth + total_from_r + total_from_l) / (3 + gamma)\n",
    "# next_depth = np.cbrt((total_from_r + pc) * (total_from_l + pc) * (depth + pc)) - pc\n",
    "# depth = (\n",
    "#     np.exp(\n",
    "#         (\n",
    "#             np.log(total_from_r + pc) +\n",
    "#             np.log(total_from_l + pc) +\n",
    "#             np.log(depth + pc)\n",
    "#         ) / (3 + pc)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Update depth\n",
    "gamma = 1.0\n",
    "nu = 0.\n",
    "\n",
    "next_depth = (total_from_r + total_from_l + (gamma * depth) + (nu * depth0)) / (2 + gamma + nu)\n",
    "# Scale the depth so there's no overall loss.\n",
    "depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "# Step done\n",
    "\n",
    "# sns.heatmap(send_to_r)\n",
    "# plt.plot(depth)\n",
    "# plt.ylim(0)\n",
    "depth\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lonely-stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = {\n",
    "    # Top cycle\n",
    "    'GGACC': [],\n",
    "}\n",
    "downstream, upstream = lib.assembly_graph.build_full_from_seed_graph(seed)\n",
    "\n",
    "observed_depth = lib.assembly_graph.add_reverse_complement_depth({\n",
    "    # Top cycle\n",
    "    'GGACC': 1,\n",
    "})\n",
    "\n",
    "assert lib.assembly_graph.mapping_all_upstream(upstream)\n",
    "upstream, observed_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth0 = pd.Series(observed_depth).astype(float)\n",
    "n = len(depth0)\n",
    "depth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Directed\" graph with each node pointing to nodes that are downstream.\n",
    "# If I want a graph of each node pointing upstream, that's just dgraph.T\n",
    "# After the *first* step, where I distribute each nodes depth equally upstream\n",
    "# and downstream, I'm going to need two contingency tables, because\n",
    "# uflow != dflow.T\n",
    "\n",
    "linked_5p_to_3p = downstream\n",
    "\n",
    "dgraph_5p_to_3p = pd.DataFrame(np.zeros((n, n)), index=depth0.index, columns=depth0.index)\n",
    "for k in dgraph_5p_to_3p:\n",
    "    dgraph_5p_to_3p.loc[k, linked_5p_to_3p[k]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_contingency = dgraph_5p_to_3p.copy()\n",
    "r_contingency = dgraph_5p_to_3p.T.copy()\n",
    "\n",
    "sns.heatmap(l_contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "send_to_r = r_contingency\n",
    "send_to_l = l_contingency\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "depth = depth0\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not initialization\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "# Initialization done\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converge first message\n",
    "\n",
    "# This is not actually the flow,\n",
    "# because it's not proportional to depth...yet.\n",
    "\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# When it's not finding the first message\n",
    "# we'll update depth here.\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "sns.heatmap(send_to_r)\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Step\n",
    "total_from_l = send_to_r.sum()\n",
    "total_from_r = send_to_l.sum()\n",
    "\n",
    "# Graveyard of depths past.\n",
    "# depth = np.sqrt(np.mean((np.array([total_from_r, total_from_l, depth]) + 1)**2))  #  (depth0 * gamma + depth + total_from_r + total_from_l) / (3 + gamma)\n",
    "# next_depth = np.cbrt((total_from_r + pc) * (total_from_l + pc) * (depth + pc)) - pc\n",
    "# depth = (\n",
    "#     np.exp(\n",
    "#         (\n",
    "#             np.log(total_from_r + pc) +\n",
    "#             np.log(total_from_l + pc) +\n",
    "#             np.log(depth + pc)\n",
    "#         ) / (3 + pc)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Update depth\n",
    "gamma = 1.0\n",
    "nu = 0.\n",
    "\n",
    "next_depth = (total_from_r + total_from_l + (gamma * depth) + (nu * depth0)) / (2 + gamma + nu)\n",
    "# Scale the depth so there's no overall loss.\n",
    "depth = next_depth * (depth.sum() / next_depth.sum())\n",
    "\n",
    "send_to_r_next = (send_to_l / total_from_r).multiply(depth, axis=1).T\n",
    "send_to_l_next = (send_to_r / total_from_l).multiply(depth, axis=1).T\n",
    "\n",
    "send_to_r = send_to_r_next.fillna(0)\n",
    "send_to_l = send_to_l_next.fillna(0)\n",
    "\n",
    "# Step done\n",
    "\n",
    "# sns.heatmap(send_to_r)\n",
    "# plt.plot(depth)\n",
    "# plt.ylim(0)\n",
    "depth\n",
    "\n",
    "# table = pd.DataFrame(dict(depth=depth, in_degree_r=total_from_r, in_degree_l=total_from_l, out_degree_r=send_to_r.sum(1), out_degree_l=send_to_l.sum(1)))\n",
    "# table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}